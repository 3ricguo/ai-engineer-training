from enum import Enum
import json
import os
from pathlib import Path
from re import split
import time
from typing import Any, Dict, List

from dotenv import load_dotenv
from llama_index.core import Document, Settings, SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.evaluation import RelevancyEvaluator
from llama_index.core.node_parser import (
    SentenceSplitter,
    SentenceWindowNodeParser,
    TokenTextSplitter,
)
from llama_index.core.postprocessor import MetadataReplacementPostProcessor
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.schema import TextNode
from llama_index.embeddings.dashscope import (
    DashScopeEmbedding,
    DashScopeTextEmbeddingModels,
)
from llama_index.llms.dashscope import DashScope
from llama_index.readers.file import MarkdownReader


load_dotenv(".env")


class SplitterType(Enum):
    """åˆ‡ç‰‡æ–¹æ³•æšä¸¾"""

    SENTENCE = "sentence"
    TOKEN = "token"
    SENTENCE_WINDOW = "sentence_window"

    @property
    def display_name(self) -> str:
        """è¿”å›æ˜¾ç¤ºåç§°"""
        display_names = {
            SplitterType.SENTENCE: "å¥å­åˆ‡ç‰‡",
            SplitterType.TOKEN: "Tokenåˆ‡ç‰‡",
            SplitterType.SENTENCE_WINDOW: "å¥å­çª—å£åˆ‡ç‰‡",
        }
        return display_names[self]

    @classmethod
    def from_display_name(cls, display_name: str) -> "SplitterType":
        """ä»æ˜¾ç¤ºåç§°è·å–æšä¸¾å€¼"""
        name_mapping = {
            "å¥å­åˆ‡ç‰‡": cls.SENTENCE,
            "Tokenåˆ‡ç‰‡": cls.TOKEN,
            "å¥å­çª—å£åˆ‡ç‰‡": cls.SENTENCE_WINDOW,
        }
        return name_mapping.get(display_name)


def setup_environment():
    """é…ç½®LlamaIndexç¯å¢ƒå’Œæ¨¡å‹"""
    # è®¾ç½®DashScope API Key
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        print("è­¦å‘Š: æœªè®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡")
        print("è¯·è®¾ç½®: export DASHSCOPE_API_KEY='your_api_key'")
        return False

    # é…ç½®LLM
    llm = DashScope(model_name="qwen-plus", api_key=api_key, max_tokens=1024)
    # DashScope(model_name="qwen-turbo", api_key=api_key, max_tokens=1024)

    # é…ç½®åµŒå…¥æ¨¡å‹
    embed_model = DashScopeEmbedding(
        model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1, api_key=api_key
    )

    # è®¾ç½®å…¨å±€é…ç½®
    Settings.llm = llm
    Settings.embed_model = embed_model
    Settings.chunk_size = 512
    Settings.chunk_overlap = 50

    print("âœ… ç¯å¢ƒé…ç½®å®Œæˆ")
    return True


def load_documents() -> List[Document]:
    """åŠ è½½æµ‹è¯•æ–‡æ¡£"""
    documents = []
    data_dir = Path(__file__).parent / "data" / "documents"

    parser = MarkdownReader()
    file_extractor = {".md": parser}
    documents = SimpleDirectoryReader(
        data_dir, file_extractor=file_extractor
    ).load_data()

    # for file_path in data_dir.glob("*.md"):
    #     try:
    #         with open(file_path, "r", encoding="utf-8") as f:
    #             content = f.read()

    #         # è¿‡æ»¤æ‰å¯¼èˆªå’Œç•Œé¢å…ƒç´ ï¼Œåªä¿ç•™å®é™…å†…å®¹
    #         lines = content.split("\n")
    #         filtered_lines = []

    #         for line in lines:
    #             # è·³è¿‡ç»´åŸºç™¾ç§‘çš„å¯¼èˆªã€é“¾æ¥ç­‰ç•Œé¢å…ƒç´ 
    #             if any(
    #                 skip_word in line.lower()
    #                 for skip_word in [
    #                     "wikipedia",
    #                     "ç»´åŸºç™¾ç§‘",
    #                     "ç¼–è¾‘",
    #                     "è®¨è®º",
    #                     "æŸ¥çœ‹",
    #                     "å·¥å…·",
    #                     "accesskey",
    #                     "hreflang",
    #                     'class="',
    #                     'href="',
    #                     "![",
    #                     "ç§»è‡³ä¾§æ ",
    #                     "éšè—",
    #                     "å¤–è§‚",
    #                     "æ‰“å°",
    #                     "ä¸‹è½½",
    #                 ]
    #             ):
    #                 continue

    #             # ä¿ç•™æœ‰å®é™…å†…å®¹çš„è¡Œ
    #             if (
    #                 len(line.strip()) > 10
    #                 and not line.startswith("|")
    #                 and not line.startswith("+")
    #             ):
    #                 filtered_lines.append(line)

    #         filtered_content = "\n".join(filtered_lines)

    #         # åªæœ‰å½“è¿‡æ»¤åçš„å†…å®¹è¶³å¤Ÿé•¿æ—¶æ‰æ·»åŠ 
    #         if len(filtered_content) > 500:
    #             doc = Document(
    #                 text=filtered_content,
    #                 metadata={"filename": file_path.name, "source": str(file_path)},
    #             )
    #             documents.append(doc)
    #             print(
    #                 f"âœ… åŠ è½½æ–‡æ¡£: {file_path.name} (é•¿åº¦: {len(filtered_content)}å­—ç¬¦)"
    #             )

    #     except Exception as e:
    #         print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {file_path.name}: {e}")

    print(f"ğŸ“š æ€»å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


def create_splitter(splitter_type: SplitterType, **kwargs):
    """æ ¹æ®æšä¸¾ç±»å‹åˆ›å»ºå¯¹åº”çš„åˆ‡ç‰‡å™¨"""
    if splitter_type == SplitterType.SENTENCE:
        return create_sentence_splitter(**kwargs)
    elif splitter_type == SplitterType.TOKEN:
        return create_token_splitter(**kwargs)
    elif splitter_type == SplitterType.SENTENCE_WINDOW:
        return create_sentence_window_splitter(**kwargs)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„åˆ‡ç‰‡å™¨ç±»å‹: {splitter_type}")


def create_sentence_splitter(
    chunk_size: int = 512, chunk_overlap: int = 50
) -> SentenceSplitter:
    """åˆ›å»ºå¥å­åˆ‡ç‰‡å™¨"""
    # é™åˆ¶chunk_sizeä¸è¶…è¿‡åµŒå…¥æ¨¡å‹é™åˆ¶
    max_chunk_size = 2000  # ç•™ä¸€äº›ä½™é‡
    if chunk_size > max_chunk_size:
        print(f"è­¦å‘Š: chunk_size {chunk_size} è¶…è¿‡é™åˆ¶ï¼Œè°ƒæ•´ä¸º {max_chunk_size}")
        chunk_size = max_chunk_size

    return SentenceSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separator=" ",
    )


def create_token_splitter(
    chunk_size: int = 512, chunk_overlap: int = 50
) -> TokenTextSplitter:
    """åˆ›å»ºTokenåˆ‡ç‰‡å™¨"""
    # é™åˆ¶chunk_sizeä¸è¶…è¿‡åµŒå…¥æ¨¡å‹é™åˆ¶
    max_chunk_size = 2000  # ç•™ä¸€äº›ä½™é‡
    if chunk_size > max_chunk_size:
        print(f"è­¦å‘Š: chunk_size {chunk_size} è¶…è¿‡é™åˆ¶ï¼Œè°ƒæ•´ä¸º {max_chunk_size}")
        chunk_size = max_chunk_size

    return TokenTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separator=" ",
    )


def create_sentence_window_splitter(
    window_size: int = 3, window_metadata_key: str = "window"
) -> SentenceWindowNodeParser:
    """åˆ›å»ºå¥å­çª—å£åˆ‡ç‰‡å™¨"""
    # éœ€è¦å…ˆåˆ›å»ºä¸€ä¸ªåŸºç¡€çš„å¥å­åˆ‡ç‰‡å™¨
    sentence_splitter = SentenceSplitter(
        chunk_size=1024,  # è®¾ç½®è¾ƒå¤§çš„chunk_size
        chunk_overlap=20,
    )

    return SentenceWindowNodeParser(window_size=window_size)


def validate_and_clean_nodes(nodes_list, splitter_type, max_length=2048, min_length=1):
    """éªŒè¯å¹¶æ¸…ç†èŠ‚ç‚¹ï¼Œç¡®ä¿æ–‡æœ¬é•¿åº¦ç¬¦åˆåµŒå…¥æ¨¡å‹è¦æ±‚

    Args:
        nodes_list: èŠ‚ç‚¹åˆ—è¡¨
        splitter_type: åˆ‡ç‰‡å™¨ç±»å‹
        max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦é™åˆ¶
        min_length: æœ€å°æ–‡æœ¬é•¿åº¦é™åˆ¶

    Returns:
        List[TextNode]: æ¸…ç†åçš„æœ‰æ•ˆèŠ‚ç‚¹åˆ—è¡¨
    """
    cleaned_nodes = []

    for i, node in enumerate(nodes_list):
        try:
            text = node.text.strip() if hasattr(node, "text") else str(node)

            # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
            if len(text) < min_length:
                print(f"è·³è¿‡èŠ‚ç‚¹ {i}: æ–‡æœ¬å¤ªçŸ­ (é•¿åº¦: {len(text)})")
                continue

            if len(text) > max_length:
                # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
                original_length = len(text)
                text = text[:max_length]
                print(f"æˆªæ–­èŠ‚ç‚¹ {i}: {original_length} -> {len(text)} å­—ç¬¦")

                # æ›´æ–°èŠ‚ç‚¹æ–‡æœ¬
                if hasattr(node, "text"):
                    node.text = text
                else:
                    # å¦‚æœä¸æ˜¯æ ‡å‡†èŠ‚ç‚¹ï¼Œåˆ›å»ºæ–°çš„TextNode
                    from llama_index.core.schema import TextNode

                    node = TextNode(
                        text=text,
                        id_=f"{splitter_type.value}_truncated_node_{i}",
                        metadata=getattr(node, "metadata", {}),
                    )

            cleaned_nodes.append(node)

        except Exception as e:
            print(f"å¤„ç†èŠ‚ç‚¹ {i} æ—¶å‡ºé”™: {e}")
            continue

    return cleaned_nodes


def evaluate_splitter(
    documents: List[Document],
    splitter,
    splitter_type: SplitterType,
    chunk_size: int = 512,
    chunk_overlap: int = 50,
) -> Dict[str, Any]:
    """è¯„ä¼°åˆ‡ç‰‡æ–¹æ³•çš„æ•ˆæœ"""
    splitter_name = splitter_type.display_name
    print(f"\nğŸ” è¯„ä¼°åˆ‡ç‰‡æ–¹æ³•: {splitter_name}")

    # æ ¹æ®åˆ‡ç‰‡å™¨ç±»å‹æ˜¾ç¤ºä¸åŒçš„å‚æ•°ä¿¡æ¯
    if splitter_type == SplitterType.SENTENCE_WINDOW:
        print(f"å‚æ•°: window_size={getattr(splitter, 'window_size', 3)}")
    else:
        print(f"å‚æ•°: chunk_size={chunk_size}, chunk_overlap={chunk_overlap}")

    start_time = time.time()

    try:
        # åˆ›å»ºèŠ‚ç‚¹
        if splitter_type == SplitterType.SENTENCE_WINDOW:
            # ä¸ºSentenceWindowNodeParseræ·»åŠ ç‰¹æ®Šå¤„ç†
            print("Processing documents for SentenceWindowNodeParser...")
            for i, doc in enumerate(documents):
                print(f"Document {i}: type={type(doc)}, has_id={hasattr(doc, 'id_')}")
                if not hasattr(doc, "id_") or not doc.id_:
                    doc.id_ = f"doc_{i}"

        nodes = splitter.get_nodes_from_documents(documents)

        # å…³é”®ä¿®å¤ï¼šæ£€æŸ¥nodesæ˜¯å¦ä¸ºNone
        if nodes is None:
            print(f"âŒ é”™è¯¯: {splitter_name} è¿”å›äº† None")
            return {
                "splitter_name": splitter_name,
                "splitter_type": splitter_type.value,
                "error": "Splitter returned None - check documents and splitter configuration",
                "processing_time": time.time() - start_time,
                "node_count": 0,
                "avg_node_length": 0,
                "responses": [],
            }

        # æ£€æŸ¥nodesæ˜¯å¦ä¸ºç©ºåˆ—è¡¨
        if not isinstance(nodes, list):
            print(f"âŒ é”™è¯¯: {splitter_name} è¿”å›äº† {type(nodes)}ï¼ŒæœŸæœ›åˆ—è¡¨")
            return {
                "splitter_name": splitter_name,
                "splitter_type": splitter_type.value,
                "error": f"Splitter returned {type(nodes)}, expected list",
                "processing_time": time.time() - start_time,
                "node_count": 0,
                "avg_node_length": 0,
                "responses": [],
            }

        print(f"åŸå§‹èŠ‚ç‚¹æ•°é‡: {len(nodes)}")
        print(f"ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ç±»å‹: {type(nodes[0]) if nodes else 'None'}")

        # éªŒè¯èŠ‚ç‚¹æ ¼å¼å¹¶ä¿®å¤
        valid_nodes = []
        for i, node in enumerate(nodes):
            if isinstance(node, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„Nodeå¯¹è±¡
                from llama_index.core.schema import TextNode

                new_node = TextNode(
                    text=node,
                    id_=f"{splitter_type.value}_node_{i}",
                    metadata={"source": f"document_{i // 10}", "chunk_id": i},
                )
                valid_nodes.append(new_node)
            elif hasattr(node, "text") and hasattr(node, "id_"):
                # å¦‚æœæ˜¯æ­£ç¡®çš„Nodeå¯¹è±¡ï¼Œç¡®ä¿æœ‰id_
                if not node.id_:
                    node.id_ = f"{splitter_type.value}_node_{i}"
                valid_nodes.append(node)
            else:
                # å¤„ç†å…¶ä»–ç±»å‹çš„èŠ‚ç‚¹
                print(f"è­¦å‘Š: èŠ‚ç‚¹ {i} ç±»å‹å¼‚å¸¸: {type(node)}")
                try:
                    # å°è¯•è·å–èŠ‚ç‚¹çš„æ–‡æœ¬å†…å®¹
                    if hasattr(node, "text"):
                        text_content = node.text
                    elif hasattr(node, "get_content"):
                        text_content = node.get_content()
                    else:
                        text_content = (
                            str(node) if hasattr(node, "__str__") else f"node_{i}"
                        )

                    new_node = TextNode(
                        text=text_content,
                        id_=f"{splitter_type.value}_node_{i}",
                        metadata={"source": f"document_{i // 10}", "chunk_id": i},
                    )
                    valid_nodes.append(new_node)
                except Exception as e:
                    print(f"æ— æ³•å¤„ç†èŠ‚ç‚¹ {i}: {e}")
                    continue

        nodes = valid_nodes

        # åº”ç”¨èŠ‚ç‚¹éªŒè¯å’Œæ¸…ç†ï¼ˆè°ƒç”¨æ¨¡å—çº§å‡½æ•°ï¼‰
        print(f"éªŒè¯èŠ‚ç‚¹æ–‡æœ¬é•¿åº¦...")
        nodes = validate_and_clean_nodes(nodes, splitter_type)

        if not nodes:
            print(f"âš ï¸  è­¦å‘Š: éªŒè¯åæ²¡æœ‰æœ‰æ•ˆèŠ‚ç‚¹")
            return {
                "splitter_name": splitter_name,
                "splitter_type": splitter_type.value,
                "error": "No valid nodes after text length validation",
                "processing_time": time.time() - start_time,
                "node_count": 0,
                "avg_node_length": 0,
                "responses": [],
            }

        # æ‰“å°èŠ‚ç‚¹é•¿åº¦ç»Ÿè®¡
        text_lengths = [len(node.text) for node in nodes if hasattr(node, "text")]
        if text_lengths:
            print(
                f"èŠ‚ç‚¹æ–‡æœ¬é•¿åº¦ç»Ÿè®¡: æœ€å°={min(text_lengths)}, æœ€å¤§={max(text_lengths)}, å¹³å‡={sum(text_lengths) / len(text_lengths):.1f}"
            )

        print(f"âœ… éªŒè¯å®Œæˆï¼Œæœ‰æ•ˆèŠ‚ç‚¹æ•°é‡: {len(nodes)}")

        # æ„å»ºç´¢å¼•æ—¶æ·»åŠ é”™è¯¯å¤„ç†
        try:
            # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ä¼ é€’ç»™VectorStoreIndexçš„æ˜¯æœ‰æ•ˆçš„èŠ‚ç‚¹åˆ—è¡¨
            if not isinstance(nodes, list) or not nodes:
                raise ValueError(
                    f"Invalid nodes for VectorStoreIndex: type={type(nodes)}, length={len(nodes) if hasattr(nodes, '__len__') else 'unknown'}"
                )

            # éªŒè¯æ¯ä¸ªèŠ‚ç‚¹éƒ½æ˜¯æœ‰æ•ˆçš„TextNodeä¸”æ–‡æœ¬é•¿åº¦åˆé€‚
            for i, node in enumerate(nodes):
                if not hasattr(node, "text") or not hasattr(node, "id_"):
                    raise ValueError(
                        f"Invalid node at index {i}: missing text or id_ attribute"
                    )
                if not (1 <= len(node.text) <= 2048):
                    raise ValueError(
                        f"Invalid text length at node {i}: {len(node.text)} (should be 1-2048)"
                    )

            print(f"åˆ›å»ºVectorStoreIndexï¼ŒèŠ‚ç‚¹æ•°é‡: {len(nodes)}")
            index = VectorStoreIndex(nodes)
        except Exception as e:
            print(f"âŒ æ„å»ºç´¢å¼•å¤±è´¥: {str(e)}")
            return {
                "splitter_name": splitter_name,
                "splitter_type": splitter_type.value,
                "error": f"Index creation failed: {str(e)}",
                "processing_time": time.time() - start_time,
                "node_count": len(nodes) if nodes else 0,
                "avg_node_length": sum(len(node.text) for node in nodes) / len(nodes)
                if nodes
                else 0,
                "responses": [],
            }

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        if splitter_type == SplitterType.SENTENCE_WINDOW:
            # å¥å­çª—å£åˆ‡ç‰‡éœ€è¦ç‰¹æ®Šçš„åå¤„ç†å™¨
            postprocessor = MetadataReplacementPostProcessor(
                target_metadata_key="window"
            )
            query_engine = index.as_query_engine(
                node_postprocessors=[postprocessor], similarity_top_k=3
            )
        else:
            query_engine = index.as_query_engine(similarity_top_k=3)

        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ",
            "RAGæŠ€æœ¯çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿ",
            "SCPåŸºé‡‘ä¼šæ˜¯ä»€ä¹ˆç»„ç»‡ï¼Ÿ",
        ]

        responses = []
        for query in test_queries:
            try:
                response = query_engine.query(query)
                responses.append(
                    {
                        "query": query,
                        "response": str(response),
                        "source_nodes": len(response.source_nodes)
                        if hasattr(response, "source_nodes")
                        else 0,
                    }
                )
            except Exception as e:
                print(f"æŸ¥è¯¢å¤±è´¥: {query} - {e}")
                responses.append(
                    {"query": query, "response": f"æŸ¥è¯¢å¤±è´¥: {e}", "source_nodes": 0}
                )

        processing_time = time.time() - start_time

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        node_count = len(nodes)
        avg_node_length = (
            sum(len(node.text) for node in nodes) / node_count if node_count > 0 else 0
        )

        results = {
            "splitter_name": splitter_name,
            "splitter_type": splitter_type.value,
            "parameters": {"chunk_size": chunk_size, "chunk_overlap": chunk_overlap},
            "statistics": {
                "node_count": node_count,
                "avg_node_length": round(avg_node_length, 2),
                "processing_time": round(processing_time, 2),
            },
            "test_results": responses,
        }

        print(f"âœ… èŠ‚ç‚¹æ•°é‡: {node_count}")
        print(f"âœ… å¹³å‡èŠ‚ç‚¹é•¿åº¦: {avg_node_length:.2f}å­—ç¬¦")
        print(f"âœ… å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")

        return results

    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        return {
            "splitter_name": splitter_name,
            "splitter_type": splitter_type.value,
            "error": str(e),
            "parameters": {"chunk_size": chunk_size, "chunk_overlap": chunk_overlap},
        }


def run_parameter_comparison(documents: List[Document]):
    """è¿è¡Œå‚æ•°å¯¹æ¯”å®éªŒ"""
    print("\nğŸ§ª å¼€å§‹å‚æ•°å¯¹æ¯”å®éªŒ")

    # æµ‹è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
    parameter_combinations = [
        {"chunk_size": 256, "chunk_overlap": 25},
        {"chunk_size": 512, "chunk_overlap": 50},
        {"chunk_size": 1024, "chunk_overlap": 100},
        {"chunk_size": 512, "chunk_overlap": 0},  # æ— é‡å 
        {"chunk_size": 512, "chunk_overlap": 128},  # é«˜é‡å 
    ]

    all_results = []

    for params in parameter_combinations:
        print(f"\nğŸ“Š æµ‹è¯•å‚æ•°ç»„åˆ: {params}")

        # æµ‹è¯•å¥å­åˆ‡ç‰‡
        sentence_splitter = create_splitter(SplitterType.SENTENCE, **params)
        sentence_results = evaluate_splitter(
            documents, sentence_splitter, SplitterType.SENTENCE, **params
        )
        all_results.append(sentence_results)

        # æµ‹è¯•Tokenåˆ‡ç‰‡
        token_splitter = create_splitter(SplitterType.TOKEN, **params)
        token_results = evaluate_splitter(
            documents, token_splitter, SplitterType.TOKEN, **params
        )
        all_results.append(token_results)

    # æµ‹è¯•å¥å­çª—å£åˆ‡ç‰‡ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
    window_splitter = create_splitter(SplitterType.SENTENCE_WINDOW)
    window_results = evaluate_splitter(
        documents, window_splitter, SplitterType.SENTENCE_WINDOW
    )
    all_results.append(window_results)

    return all_results


def save_results(results: List[Dict], output_file: str = "experiment_results.json"):
    """ä¿å­˜å®éªŒç»“æœ"""
    output_path = Path(__file__).parent / output_file

    try:
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")


def print_summary(results: List[Dict]):
    """æ‰“å°å®éªŒç»“æœæ‘˜è¦"""
    print("\nğŸ“‹ å®éªŒç»“æœæ‘˜è¦")
    print("=" * 60)

    for result in results:
        if "error" in result:
            print(f"âŒ {result['splitter_name']}: {result['error']}")
            continue

        stats = result.get("statistics", {})
        params = result.get("parameters", {})
        splitter_type = result.get("splitter_type", "unknown")

        print(f"\nğŸ”¸ {result['splitter_name']} ({splitter_type})")
        print(
            f"   å‚æ•°: chunk_size={params.get('chunk_size', 'N/A')}, chunk_overlap={params.get('chunk_overlap', 'N/A')}"
        )
        print(f"   èŠ‚ç‚¹æ•°: {stats.get('node_count', 'N/A')}")
        print(f"   å¹³å‡é•¿åº¦: {stats.get('avg_node_length', 'N/A')}å­—ç¬¦")
        print(f"   å¤„ç†æ—¶é—´: {stats.get('processing_time', 'N/A')}ç§’")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ–‡æœ¬åˆ‡ç‰‡å¯¹æ¯”å®éªŒ")

    # 1. é…ç½®ç¯å¢ƒ
    if not setup_environment():
        print("âŒ ç¯å¢ƒé…ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥DASHSCOPE_API_KEY")
        return

    # 2. åŠ è½½æ–‡æ¡£
    documents = load_documents()
    if not documents:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£")
        return

    # 3. è¿è¡Œå‚æ•°å¯¹æ¯”å®éªŒ
    results = run_parameter_comparison(documents)

    # 4. ä¿å­˜å’Œå±•ç¤ºç»“æœ
    save_results(results)
    print_summary(results)

    print("\nâœ… å®éªŒå®Œæˆï¼è¯·æŸ¥çœ‹experiment_results.jsonæ–‡ä»¶è·å–è¯¦ç»†ç»“æœ")
    print("ğŸ“ æ¥ä¸‹æ¥è¯·å®Œå–„report.mdæ–‡ä»¶ä¸­çš„å®éªŒåˆ†æ")


if __name__ == "__main__":
    main()
